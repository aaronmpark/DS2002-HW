{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6aee17-c9e1-475c-8807-6e72e4067ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter input format (csv/json):  csv\n",
      "Enter output format (csv/json/sql):  json\n",
      "Enter your Source File or URL:  spongebob_episodes.csv\n",
      "Enter your Destination File Name:  episodes.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion Summary:\n",
      "Number of records: 583\n",
      "Number of columns: 25\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to add some columns or delete some columns (add/delete):  add\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column Modification Summary:\n",
      "Number of records: 583\n",
      "Number of columns: 26\n",
      "\n",
      "Post-processing Summary:\n",
      "Number of records: 583\n",
      "Number of columns: 26\n",
      "ETL process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Aaron Park ync4hn\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import requests\n",
    "import sqlite3\n",
    "from typing import List, Dict\n",
    "\n",
    "class ETLProcessor:\n",
    "    # INITIALIZE VALUES\n",
    "    def __init__(self, input_format: str, output_format: str):\n",
    "        self.input_format = input_format\n",
    "        self.output_format = output_format\n",
    "        self.data = []\n",
    "\n",
    "    # 1. Fetching the data from a URL or a local remote file\n",
    "    def fetch_data(self, source: str):\n",
    "        try:\n",
    "            if source.startswith('http'): # URL\n",
    "                response = requests.get(source)\n",
    "                response.raise_for_status()\n",
    "                if self.input_format == 'csv': # csv files\n",
    "                    self.data = list(csv.DictReader(response.text.splitlines()))\n",
    "                elif self.input_format == 'json': # json files\n",
    "                    self.data = response.json()\n",
    "            else:\n",
    "                with open(source, 'r', encoding='utf-8') as file:\n",
    "                    if self.input_format == 'csv': # csv\n",
    "                        self.data = list(csv.DictReader(file))\n",
    "                    elif self.input_format == 'json': # json\n",
    "                        self.data = json.load(file)\n",
    "            \n",
    "            # Print the Ingestion Summary\n",
    "            print(f\"Ingestion Summary:\")\n",
    "            print(f\"Number of records: {len(self.data)}\")\n",
    "            print(f\"Number of columns: {len(self.data[0])}\")\n",
    "        \n",
    "        # ERROR STATEMENTS\n",
    "        except requests.RequestException as e:\n",
    "            raise Exception(f\"Error fetching data from URL: {str(e)}\")\n",
    "        except FileNotFoundError:\n",
    "            raise Exception(f\"Error: File '{source}' not found\")\n",
    "        except json.JSONDecodeError:\n",
    "            raise Exception(\"Error: Invalid JSON format\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error fetching data: {str(e)}\")\n",
    "\n",
    "    # 3. Modify the Columns from the source to the destination\n",
    "    def modify_columns(self, columns_to_add: Dict[str, str] = None, columns_to_delete: List[str] = None):\n",
    "        try: \n",
    "            # ADDING COLUMNS TO THE ORIGINAL DATA\n",
    "            if columns_to_add != None:\n",
    "                for record in self.data:\n",
    "                    record.update(columns_to_add)\n",
    "            # DELETING COLUMNS FROM THE ORIGINAL DATA\n",
    "            elif columns_to_delete != None:\n",
    "                filtered_records = []\n",
    "                for record in self.data:\n",
    "                    filtered_record = {key: value for key, value in record.items() if key not in columns_to_delete}\n",
    "                    filtered_records.append(filtered_record)\n",
    "                self.data = filtered_records         \n",
    "\n",
    "            # PRINT OUT MODIFICATION SUMMARY\n",
    "            print(f\"\\nColumn Modification Summary:\")\n",
    "            print(f\"Number of records: {len(self.data)}\")\n",
    "            print(f\"Number of columns: {len(self.data[0])}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error modifying columns: {str(e)}\")\n",
    "\n",
    "    # Convert the data AND STORE THE DATA [both 2 and 4]\n",
    "    def store_data(self, destination: str):\n",
    "        try:\n",
    "            # if output is a csv file \n",
    "            if self.output_format == 'csv':\n",
    "                with open(destination, 'w', newline='', encoding='utf-8') as file:\n",
    "                    writer = csv.DictWriter(file, fieldnames=self.data[0].keys())\n",
    "                    writer.writeheader()\n",
    "                    writer.writerows(self.data)\n",
    "            # if output is a json file [dump]\n",
    "            elif self.output_format == 'json':\n",
    "                with open(destination, 'w', encoding='utf-8') as file:\n",
    "                    json.dump(self.data, file, indent=2)\n",
    "            # if output is sql file [connect to sql and then create it through the instance]\n",
    "            elif self.output_format == 'sql':\n",
    "                conn = sqlite3.connect(destination)\n",
    "                cursor = conn.cursor()\n",
    "                # creating the table within the sql database\n",
    "                columns = ', '.join([f\"{key} TEXT\" for key in self.data[0].keys()])\n",
    "                cursor.execute(f\"CREATE TABLE IF NOT EXISTS newTable ({columns})\")\n",
    "                \n",
    "                # insert the data that we have modified\n",
    "                for record in self.data:\n",
    "                    placeholders = ', '.join(['?' for _ in record])\n",
    "                    values = [json.dumps(value) if isinstance(value, (dict, list)) else str(value) for value in record.values()]\n",
    "                    cursor.execute(f\"INSERT INTO newTable VALUES ({placeholders})\", tuple(values))\n",
    "                \n",
    "                conn.commit()\n",
    "                conn.close()\n",
    "\n",
    "            # POST PROCESSING SUMMARY\n",
    "            print(f\"\\nPost-processing Summary:\")\n",
    "            print(f\"Number of records: {len(self.data)}\")\n",
    "            print(f\"Number of columns: {len(self.data[0])}\")\n",
    "        except sqlite3.Error as e:\n",
    "            raise Exception(f\"SQLite error: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error storing data: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        input_format = input(\"Enter input format (csv/json): \").lower()\n",
    "        if input_format not in ['csv', 'json']:\n",
    "            raise ValueError(\"Invalid input format. Please choose 'csv' or 'json'.\")\n",
    "\n",
    "        output_format = input(\"Enter output format (csv/json/sql): \").lower()\n",
    "        if output_format not in ['csv', 'json', 'sql']:\n",
    "            raise ValueError(\"Invalid output format. Please choose 'csv', 'json', or 'sql'.\")\n",
    "\n",
    "        source = input(\"Enter your Source File or URL: \")\n",
    "        destination = input(\"Enter your Destination File Name: \")\n",
    "\n",
    "        processor = ETLProcessor(input_format, output_format)\n",
    "\n",
    "        processor.fetch_data(source)\n",
    "        \n",
    "        modify_format = input(\"Do you want to add some columns or delete some columns (add/delete): \").lower()\n",
    "        if modify_format == \"delete\":\n",
    "            columns_to_delete = [col.strip() for col in input(\"Enter columns to delete (comma-separated): \").split(',')]\n",
    "            columns_to_add = None\n",
    "        elif modify_format == \"add\":\n",
    "            columns_to_add = {'Processed': 'True'}  # Example of adding a column\n",
    "            columns_to_delete = None\n",
    "\n",
    "        processor.modify_columns(columns_to_add, columns_to_delete)\n",
    "        processor.store_data(destination)\n",
    "        \n",
    "        print(\"ETL process completed successfully.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0dce8-e73b-40b3-a738-6e3e3b93cafc",
   "metadata": {},
   "source": [
    "Testing Code Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7cf0ac8-aa55-4231-b39f-6f0dca6d5d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion Summary:\n",
      "Number of records: 583\n",
      "Number of columns: 25\n",
      "\n",
      "Column Modification Summary:\n",
      "Number of records: 583\n",
      "Number of columns: 23\n",
      "\n",
      "Post-processing Summary:\n",
      "Number of records: 583\n",
      "Number of columns: 23\n",
      "ETL process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# CSV TO JSON - DELETE           [add done above]\n",
    "input_format = \"csv\"\n",
    "output_format = \"json\"\n",
    "\n",
    "source = \"spongebob_episodes.csv\"\n",
    "destination = \"episodes_delete.json\"\n",
    "\n",
    "processor = ETLProcessor(input_format, output_format)\n",
    "processor.fetch_data(source)\n",
    "columns_to_delete = [\"Airdate\", \"Animation\"]\n",
    "columns_to_add = None\n",
    "processor.modify_columns(columns_to_add, columns_to_delete)\n",
    "processor.store_data(destination)\n",
    "\n",
    "print(\"ETL process completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1581f6d5-3b7a-4f3c-a10f-47f2525ea5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion Summary:\n",
      "Number of records: 583\n",
      "Number of columns: 4\n",
      "\n",
      "Column Modification Summary:\n",
      "Number of records: 583\n",
      "Number of columns: 5\n",
      "\n",
      "Post-processing Summary:\n",
      "Number of records: 583\n",
      "Number of columns: 5\n",
      "ETL process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# JSON TO SQL - ADD \n",
    "input_format = \"json\"\n",
    "output_format = \"sql\"\n",
    "\n",
    "source = \"spongebob_episodes.json\"\n",
    "destination = \"episodes_add.sql\"\n",
    "\n",
    "processor = ETLProcessor(input_format, output_format)\n",
    "processor.fetch_data(source)\n",
    "columns_to_delete = None\n",
    "columns_to_add = {\"Processed\": \"True\"}\n",
    "processor.modify_columns(columns_to_add, columns_to_delete)\n",
    "processor.store_data(destination)\n",
    "\n",
    "print(\"ETL process completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "405c93ae-062a-4a12-89df-102f1cfb28d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion Summary:\n",
      "Number of records: 583\n",
      "Number of columns: 4\n",
      "\n",
      "Column Modification Summary:\n",
      "Number of records: 583\n",
      "Number of columns: 4\n",
      "\n",
      "Post-processing Summary:\n",
      "Number of records: 583\n",
      "Number of columns: 4\n",
      "ETL process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# JSON TO SQL - DELETE\n",
    "input_format = \"json\"\n",
    "output_format = \"sql\"\n",
    "\n",
    "source = \"spongebob_episodes.json\"\n",
    "destination = \"episodes_delete.sql\"\n",
    "\n",
    "processor = ETLProcessor(input_format, output_format)\n",
    "processor.fetch_data(source)\n",
    "columns_to_delete = [\"Airdate\", \"Creative\"]\n",
    "columns_to_add = None\n",
    "processor.modify_columns(columns_to_add, columns_to_delete)\n",
    "processor.store_data(destination)\n",
    "\n",
    "print(\"ETL process completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b6f8cb-f85c-4b06-b540-0e970d9a501f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
